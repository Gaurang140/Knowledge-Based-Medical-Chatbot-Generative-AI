{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/gauranggirimeghanathi/Desktop/Projects/Knowledge-Based-Medical-Chatbot-Generative-AI/research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gauranggirimeghanathi/Desktop/Projects/Knowledge-Based-Medical-Chatbot-Generative-AI/medibot/lib/python3.10/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader , DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_pinecone import Pinecone\n",
    "\n",
    "\n",
    "# Exract data from the pdf file \n",
    "\n",
    "def load_pdf_file(data):\n",
    "\n",
    "    \"\"\"\n",
    "    load data using this function \n",
    "    \"\"\"\n",
    "\n",
    "    loader = DirectoryLoader(\n",
    "        data , \n",
    "        glob=\"*.pdf\", \n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_pdf_file(data=\"Data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into text chunks \n",
    "\n",
    "def text_splits(extracted_data):\n",
    "\n",
    "    text_splitters = RecursiveCharacterTextSplitter(\n",
    "\n",
    "        chunk_size = 500 , \n",
    "        chunk_overlap = 20\n",
    "    )\n",
    "\n",
    "    text_chunks = text_splitters.split_documents(extracted_data)\n",
    "\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenghth of text chunk is  39994\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text_chunks = text_splits(extracted_data=extracted_data)\n",
    "print(\"Lenghth of text chunk is \",len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download hugging face embedding model \n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "def download_hugging_face_embedding_model():\n",
    "    emebeddings = HuggingFaceEmbeddings(model_name = \"all-MiniLM-L6-v2\")\n",
    "    return emebeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings = download_hugging_face_embedding_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pc = Pinecone(api_key=pine_cone_api)\\n\\n\\nindex_name = \"medibot\"\\n\\npc.create_index(\\n    name=index_name,\\n    dimension=384,\\n    metric=\"cosine\", # Replace with your model metric\\n    spec=ServerlessSpec(\\n        cloud=\"aws\",\\n        region=\"us-east-1\"\\n    ) \\n)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store this embeddings results into pinrcune data base so we can retrive the information from there releted to our data  \n",
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone import ServerlessSpec\n",
    "import os \n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = os.getenv(\"PINE_CONE_API_KEY\")\n",
    "\n",
    "\"\"\"pc = Pinecone(api_key=pine_cone_api)\n",
    "\n",
    "\n",
    "index_name = \"medibot\"\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384,\n",
    "    metric=\"cosine\", # Replace with your model metric\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    ) \n",
    ")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "index_name = \"medibot\"\n",
    "\n",
    "\n",
    "docsearch = PineconeVectorStore.from_documents(\n",
    "    documents=text_chunks , \n",
    "    index_name=index_name, \n",
    "    embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXISTING INDEX\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "search = PineconeVectorStore.from_existing_index(index_name=index_name , embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver = search.as_retriever(search_type = \"similarity\" , search_kwargs = {\"k\":3} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='d21a3f4c-f7cc-46ce-b92d-f50191ead4af', metadata={'page': 55.0, 'source': 'Data/medical_book.pdf'}, page_content='Researchers, Inc. Reproduced by permission.)\\n26 GALE ENCYCLOPEDIA OF MEDICINE\\nAcne'),\n",
       " Document(id='60fea221-65ac-450b-87b7-ba5f0fce9dc2', metadata={'page': 54.0, 'source': 'Data/medical_book.pdf'}, page_content='occurs when new skin cells are laid down to replace\\ndamaged cells.\\nThe most common sites of acne are the face, chest,\\nshoulders, and back since these are the parts of the\\nbody where the most sebaceous follicles are found.\\nCauses and symptoms\\nThe exact cause of acne is unknown. Several risk\\nfactors have been identified:\\n/C15Age. Due to the hormonal changes they experience,\\nteenagers are more likely to develop acne.\\n/C15Gender. Boys have more severe acne and develop it\\nmore often than girls.'),\n",
       " Document(id='9fd26cb4-02f5-4a03-9435-1b0d211b272f', metadata={'page': 54.0, 'source': 'Data/medical_book.pdf'}, page_content='is the most common skin disease. It affects nearly 17\\nmillion people in the United States. While acne can\\narise at any age, it usually begins atpuberty and wor-\\nsens during adolescence. Nearly 85% of people\\ndevelop acne at some time between the ages of 12-25\\nyears. Up to 20% of women develop mild acne. It is\\nalso found in some newborns.\\nThe sebaceous glands lie just beneath the skin’s\\nsurface. They produce an oil called sebum, the skin’s\\nnatural moisturizer. These glands and the hair follicles')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriver.invoke(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "\n",
    "llm_model = ChatGroq(model = \"Llama-3.3-70b-Specdec\" , api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "\n",
    "    \"you are an assistant for question - answring tasks.\"\n",
    "    \"use the following pieces of retrival to answer\"\n",
    "    \"the question if you know the answer , say that you\"\n",
    "    \"dont know. use three sentence maxumzm and keep the answe concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \n",
    "    \"{context}\"\n",
    "\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "\n",
    "    [\n",
    "(\"system\" , system_prompt),\n",
    "(\"human\", \"{input}\"),\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(llm_model , prompt)\n",
    "reg_chain = create_retrieval_chain(retriver , question_answer_chain )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    " # :-    #### Adding Chat History \n",
    "#from langchain.chains import create_history_aware_retriever  # retriver will know the history\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),  # Your system-level instruction\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),  # Placeholder for history\n",
    "        (\"human\", \"{input}\"),  # User input\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(llm_model ,retriver , contextualize_q_prompt)\n",
    "\n",
    "\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain \n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm_model , contextualize_q_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever , question_answer_chain)\n",
    "\n",
    "\n",
    "\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store ={}\n",
    "\n",
    "store = {} ## store the chat history of the user and the model\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    \n",
    "    if session_id not in store : \n",
    "        store[session_id] = ChatMessageHistory()\n",
    "\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_reg_chain = RunnableWithMessageHistory(\n",
    "\n",
    "reg_chain, \n",
    "get_session_history,\n",
    "input_messages_key=\"input\",\n",
    "history_messages_key=\"chat_history\",\n",
    "output_messages_key=\"answer\"\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"chat1\"}} \n",
    "\n",
    "responce = conversational_reg_chain.invoke(\n",
    "\n",
    "    {\"input\": \"what is task decomposition?\"} , \n",
    "    config=config,\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know what task decomposition is. The provided text does not mention task decomposition. It discusses various medical and psychological terms, but does not define task decomposition.\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
